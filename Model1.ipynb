{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnS9TQod61K2"
      },
      "source": [
        "Initial Setup !! takes artound 20 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'pifuhd'...\n",
            "remote: Enumerating objects: 222, done.\u001b[K\n",
            "remote: Counting objects: 100% (126/126), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 222 (delta 92), reused 82 (delta 82), pack-reused 96\u001b[K\n",
            "Receiving objects: 100% (222/222), 399.35 KiB | 0 bytes/s, done.\n",
            "Resolving deltas: 100% (114/114), done.\n",
            "Cloning into 'lightweight-human-pose-estimation.pytorch'...\n",
            "remote: Enumerating objects: 120, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 120 (delta 19), reused 18 (delta 18), pack-reused 90\u001b[K\n",
            "Receiving objects: 100% (120/120), 226.05 KiB | 0 bytes/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/facebookresearch/pifuhd\n",
        "# !git clone https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch.git "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# %cd /content/lightweight-human-pose-estimation.pytorch/\n",
        "# !wget https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\n",
        "\n",
        "# %cd /content/pifuhd/\n",
        "# !sh ./scripts/download_trained_model.sh\n",
        "\n",
        "# !pip install 'torch' -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install 'torchvision' -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install \"git+https://github.com/facebookresearch/pytorch3d.git\"\n",
        "# # !pip install pytorch3d\n",
        "# !pip install rembg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/mnt/NFS/patidarritesh/vid3avatar'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os \n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/mnt/NFS/patidarritesh/vid3avatar\n"
          ]
        }
      ],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/mnt/NFS/patidarritesh/vid3avatar'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/mnt/NFS/patidarritesh/vid3avatar\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from with_mobilenet import PoseEstimationWithMobileNet\n",
        "from keypoints import extract_keypoints, group_keypoints\n",
        "from load_state import load_state\n",
        "from pose import Pose, track_poses\n",
        "import demo\n",
        "from rembg import remove\n",
        "from PIL import Image\n",
        "from IPython.display import clear_output\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDe8o_Zo6jyi",
        "outputId": "d4cfb20e-2fd5-40d3-8af8-2cfbf6a835f0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_rect(net, images, height_size):\n",
        "    net = net.eval()\n",
        "\n",
        "    stride = 8\n",
        "    upsample_ratio = 4\n",
        "    num_keypoints = Pose.num_kpts\n",
        "    previous_poses = []\n",
        "    delay = 33\n",
        "    for image in images:\n",
        "        rect_path = image.replace('.%s' % (image.split('.')[-1]), '_rect.txt')\n",
        "        img = cv2.imread(image, cv2.IMREAD_COLOR)\n",
        "        orig_img = img.copy()\n",
        "        orig_img = img.copy()\n",
        "        heatmaps, pafs, scale, pad = demo.infer_fast(net, img, height_size, stride, upsample_ratio, cpu=False)\n",
        "\n",
        "        total_keypoints_num = 0\n",
        "        all_keypoints_by_type = []\n",
        "        for kpt_idx in range(num_keypoints):  # 19th for bg\n",
        "            total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n",
        "\n",
        "        pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs)\n",
        "        for kpt_id in range(all_keypoints.shape[0]):\n",
        "            all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n",
        "            all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n",
        "        current_poses = []\n",
        "\n",
        "        rects = []\n",
        "        for n in range(len(pose_entries)):\n",
        "            if len(pose_entries[n]) == 0:\n",
        "                continue\n",
        "            pose_keypoints = np.ones((num_keypoints, 2), dtype=np.int32) * -1\n",
        "            valid_keypoints = []\n",
        "            for kpt_id in range(num_keypoints):\n",
        "                if pose_entries[n][kpt_id] != -1.0:  # keypoint was found\n",
        "                    pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n",
        "                    pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n",
        "                    valid_keypoints.append([pose_keypoints[kpt_id, 0], pose_keypoints[kpt_id, 1]])\n",
        "            valid_keypoints = np.array(valid_keypoints)\n",
        "\n",
        "            if pose_entries[n][10] != -1.0 or pose_entries[n][13] != -1.0:\n",
        "              pmin = valid_keypoints.min(0)\n",
        "              pmax = valid_keypoints.max(0)\n",
        "\n",
        "              center = (0.5 * (pmax[:2] + pmin[:2])).astype(np.int32)\n",
        "              radius = int(0.65 * max(pmax[0]-pmin[0], pmax[1]-pmin[1]))\n",
        "            elif pose_entries[n][10] == -1.0 and pose_entries[n][13] == -1.0 and pose_entries[n][8] != -1.0 and pose_entries[n][11] != -1.0:\n",
        "              # if leg is missing, use pelvis to get cropping\n",
        "              center = (0.5 * (pose_keypoints[8] + pose_keypoints[11])).astype(np.int32)\n",
        "              radius = int(1.45*np.sqrt(((center[None,:] - valid_keypoints)**2).sum(1)).max(0))\n",
        "              center[1] += int(0.05*radius)\n",
        "            else:\n",
        "              center = np.array([img.shape[1]//2,img.shape[0]//2])\n",
        "              radius = max(img.shape[1]//2,img.shape[0]//2)\n",
        "\n",
        "            x1 = center[0] - radius\n",
        "            y1 = center[1] - radius\n",
        "\n",
        "            rects.append([x1, y1, 2*radius, 2*radius])\n",
        "\n",
        "        np.savetxt(rect_path, np.array(rects), fmt='%d')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mLsMSlT65X1"
      },
      "source": [
        "Phase 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8uHAo8b7D38",
        "outputId": "b4d1bdf6-5613-41c7-8110-75d643c847a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data from 'https://github.com/danielgatis/rembg/releases/download/v0.0.0/u2net.onnx' to file '/home/patidarritesh/.u2net/u2net.onnx'.\n",
            "100%|████████████████████████████████████████| 176M/176M [00:00<00:00, 158GB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "image_path = \"vid2avtar2.jpg\"\n",
        "# Store path of the output image in the variable output_path\n",
        "output_path = 'vid2avtar2_bg_removed.png'\n",
        "# Processing the image\n",
        "input = Image.open(image_path)\n",
        "# Removing the background from the given Image\n",
        "output = remove(input)\n",
        "#Saving the image in the given path\n",
        "output.save(output_path)\n",
        "\n",
        "image_path = output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4vhfDsT7F3f"
      },
      "source": [
        "Phase 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1srias1UG5sw",
        "outputId": "97d4fd93-11d8-4c7b-f15a-c0a32e5d5427"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %cd '/content/sample_data'\n",
        "# #upload the image\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn])))\n",
        "# filename = list(uploaded.keys())[0]\n",
        "# image_path = '/content/sample_data/' + filename\n",
        "\n",
        "#empty the folder\n",
        "# !rm -rf '/content/pifuhd/sample_images'\n",
        "# !mkdir '/content/pifuhd/sample_images'\n",
        "import cv2\n",
        "\n",
        "img = cv2.imread(image_path)\n",
        "cv2.imwrite('Img.png', img)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l3j6YV-RG8YD"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "image_path = os.path.join(os.getcwd(), 'Img.png')\n",
        "# image_path = '/content/pifuhd/sample_images/Img.png'\n",
        "image_dir = os.path.dirname(image_path)\n",
        "file_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "\n",
        "# output pathes\n",
        "obj_path = 'results/pifuhd_final/recon/result_%s_256.obj' % file_name\n",
        "out_img_path = 'results/pifuhd_final/recon/result_%s_256.png' % file_name\n",
        "video_path = 'results/pifuhd_final/recon/result_%s_256.mp4' % file_name\n",
        "video_display_path = 'results/pifuhd_final/result_%s_256_display.mp4' % file_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBH3ly9-HANG",
        "outputId": "09e8f958-0903-4389-b1ab-c714587c94c9"
      },
      "outputs": [],
      "source": [
        "\n",
        "#preprosessing\n",
        "\n",
        "net = PoseEstimationWithMobileNet()\n",
        "checkpoint = torch.load('checkpoint_iter_370000.pth', map_location='cpu')\n",
        "load_state(net, checkpoint)\n",
        "get_rect(net.cuda(), [image_path], 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juv3BaO56r4T",
        "outputId": "fe116240-c3b5-4bff-d0ca-65b093ff6b6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming from  ./pifuhd.pt\n",
            "Warning: opt is overwritten.\n",
            "test data size:  1\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "generate mesh (test) ...\n",
            "  0%|                                                     | 0/1 [00:00<?, ?it/s]./results/pifuhd_final/recon/result_Img_256.obj\n",
            "\n",
            "\n",
            "mesh_path: ./results/pifuhd_final/recon/result_Img_256.obj\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:18<00:00, 19.00s/it]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#run\n",
        "# %cd /content/pifuhd/\n",
        "\n",
        "# Warning: all images with the corresponding rectangle files under -i will be processed.\n",
        "!python -m simple_test -r 256 --use_rect -i $image_dir\n",
        "# seems that 256 is the maximum resolution that can fit into Google Colab.\n",
        "# If you want to reconstruct a higher-resolution mesh, please try with your own machine.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djgKCvNU6r0b"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/pifuhd/results/pifuhd_final/recon/result_Img_256.obj')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EboWPHy87IDw"
      },
      "source": [
        "Analyse the measuremrnts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGjSYRqS6rx4"
      },
      "outputs": [],
      "source": [
        "!pip install pywavefront\n",
        "!pip install body_measurements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoxpqQkZ6wcL"
      },
      "outputs": [],
      "source": [
        "import pywavefront\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from body_measurements.measurement import Body3D\n",
        "\n",
        "\n",
        "def make_measurement(file_path):\n",
        "    person = pywavefront.Wavefront(\n",
        "        file_path,\n",
        "        create_materials=True,\n",
        "        collect_faces=True\n",
        "    )\n",
        "    faces = np.array(person.mesh_list[0].faces)\n",
        "    vertices = np.array(person.vertices)\n",
        "\n",
        "    body = Body3D(vertices, faces)\n",
        "\n",
        "    body_measurements = body.getMeasurements()\n",
        "\n",
        "    height = body.height()\n",
        "    weight = body.weight()\n",
        "    shoulder_2d, shoulder_location, shoulder_length = body.shoulder()\n",
        "    chest_2d, chest_location, chest_length = body.chest()\n",
        "    hip_2d, hip_location, hip_length = body.hip()\n",
        "    waist_2d, waist_location, waist_length = body.waist()\n",
        "    thigh_2d, thigh_location, thigh_length = body.thighOutline()\n",
        "    outer_leg_length = body.outerLeg()\n",
        "    inner_leg_length = body.innerLeg()\n",
        "    neck_2d, neck_location, neck_length = body.neck()\n",
        "    neck_hip_length = body.neckToHip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGhU6EaH6wZl"
      },
      "outputs": [],
      "source": [
        "make_measurement('/content/pifuhd/results/pifuhd_final/recon/result_Img_256.obj')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
